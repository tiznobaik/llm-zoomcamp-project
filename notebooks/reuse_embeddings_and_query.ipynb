{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3a339c5",
   "metadata": {},
   "source": [
    "# Skip Reindexing and Reuse Saved Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5dc3f2",
   "metadata": {},
   "source": [
    "\n",
    "In this notebook, we will skip the embedding generation and re-indexing process by loading previously saved document embeddings and metadata from a JSON file. We will then use this data to directly query Elasticsearch and the RetrievalQA chain.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7455d4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings and metadata from JSON.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from elasticsearch import Elasticsearch\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import Document\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "\n",
    "# Load the saved embeddings and metadata\n",
    "with open('document_embeddings.json', 'r') as f:\n",
    "    saved_data = json.load(f)\n",
    "\n",
    "# Extract texts, embeddings, and metadata\n",
    "texts = [item['text'] for item in saved_data]\n",
    "embeddings = [np.array(item['embedding']) for item in saved_data]\n",
    "metadatas = [item['metadata'] for item in saved_data]\n",
    "\n",
    "print(\"Loaded embeddings and metadata from JSON.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66186118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'fcra_chunks' does not exist. Re-indexing is required.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Elasticsearch connection\n",
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200, 'scheme': 'http'}])\n",
    "index_name = 'fcra_chunks'\n",
    "\n",
    "# Check if the index already exists\n",
    "if es.indices.exists(index=index_name):\n",
    "    print(f\"Index '{index_name}' already exists in Elasticsearch. No re-indexing required.\")\n",
    "else:\n",
    "    print(f\"Index '{index_name}' does not exist. Re-indexing is required.\")\n",
    "    index_name = 'fcra_chunks'\n",
    "\n",
    "    # Delete the index if it already exists (optional)\n",
    "    if es.indices.exists(index=index_name):\n",
    "        es.indices.delete(index=index_name)\n",
    "\n",
    "    # Define the mapping\n",
    "    mapping = {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"embedding\": {\n",
    "                    \"type\": \"dense_vector\",\n",
    "                    \"dims\": 1536\n",
    "                },\n",
    "                \"text\": {\n",
    "                    \"type\": \"text\"\n",
    "                },\n",
    "                \"metadata\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"enabled\": True\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Create the index with the mapping\n",
    "    es.indices.create(index=index_name, body=mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a08c8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bs/rk6rcnf923l4k8xms3k927hr0000gn/T/ipykernel_6956/3451141637.py:5: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding_model = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n"
     ]
    }
   ],
   "source": [
    "#  Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the embedding model with the API key\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b6b0327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bs/rk6rcnf923l4k8xms3k927hr0000gn/T/ipykernel_6956/4229264796.py:9: DeprecationWarning: Retrievers must implement abstract `_get_relevant_documents` method instead of `get_relevant_documents`\n",
      "  class ElasticSearchRetriever(BaseRetriever, BaseModel):\n"
     ]
    }
   ],
   "source": [
    "# Create a Custom Retriever Using Elasticsearch\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain.schema import BaseRetriever\n",
    "from typing import Any, List\n",
    "import numpy as np\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class ElasticSearchRetriever(BaseRetriever, BaseModel):\n",
    "    es: Any\n",
    "    index_name: str\n",
    "    embedding_model: Any\n",
    "    k: int = 5\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # Generate and normalize the query embedding\n",
    "        query_embedding = self.embedding_model.embed_query(query)\n",
    "        query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "\n",
    "        # Build the script score query\n",
    "        script_query = {\n",
    "            \"size\": self.k,\n",
    "            \"query\": {\n",
    "                \"script_score\": {\n",
    "                    \"query\": {\"match_all\": {}},\n",
    "                    \"script\": {\n",
    "                        \"source\": \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n",
    "                        \"params\": {\n",
    "                            \"query_vector\": query_embedding.tolist()\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"_source\": [\"text\", \"metadata\"]\n",
    "        }\n",
    "\n",
    "        # Execute the search\n",
    "        response = self.es.search(index=self.index_name, body=script_query)\n",
    "\n",
    "        # Convert hits to Documents\n",
    "        docs = []\n",
    "        for hit in response['hits']['hits']:\n",
    "            doc = Document(\n",
    "                page_content=hit['_source']['text'],\n",
    "                metadata=hit['_source']['metadata']\n",
    "            )\n",
    "            docs.append(doc)\n",
    "        return docs\n",
    "\n",
    "# Initialize the Retriever\n",
    "retriever = ElasticSearchRetriever(\n",
    "    es=es,\n",
    "    index_name=index_name,\n",
    "    embedding_model=embedding_model,\n",
    "    k=5  # Number of documents to retrieve\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "753d7b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bs/rk6rcnf923l4k8xms3k927hr0000gn/T/ipykernel_6956/3825728049.py:2: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
      "/var/folders/bs/rk6rcnf923l4k8xms3k927hr0000gn/T/ipykernel_6956/3825728049.py:13: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  answer = qa_chain.run(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \n",
      "The FCRA, or Fair Credit Reporting Act, is a federal law that regulates the collection, dissemination, and use of consumer credit information. Some key provisions of the FCRA include:\n",
      "\n",
      "1. Disclosure requirements: The FCRA requires that consumers be informed of their rights and the information being collected about them.\n",
      "\n",
      "2. Accuracy of information: The FCRA mandates that credit reporting agencies maintain accurate and up-to-date information about consumers.\n",
      "\n",
      "3. Dispute resolution: The FCRA gives consumers the right to dispute and correct any errors on their credit reports.\n",
      "\n",
      "4. Access to credit reports: Consumers have the right to access their credit reports and receive a free copy once every 12 months.\n",
      "\n",
      "5. Adverse action notices: Lenders and creditors are required to provide consumers with a notice if they take any adverse action based on information from their credit report.\n",
      "\n",
      "6. Limitations on use of credit reports: The FCRA restricts who can access a consumer's credit report and for what purposes.\n",
      "\n",
      "7. Identity theft prevention: The FCRA includes provisions to help protect consumers from identity theft and requires credit reporting agencies to place fraud alerts on credit reports.\n",
      "\n",
      "8. Enforcement: The FCRA allows for enforcement by both federal and state agencies, as well as private lawsuits by consumers.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the LLM (GPT)\n",
    "llm = OpenAI(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Create the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\",  # You can change to map_reduce, etc.\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "# Run a test query\n",
    "query = \"What are the key provisions of the FCRA?\"\n",
    "answer = qa_chain.run(query)\n",
    "print(\"Answer:\", answer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1d6dd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the permissible purposes for obtaining a consumer report?\n",
      "Answer: \n",
      "\n",
      "The permissible purposes for obtaining a consumer report include employment purposes, credit and loan applications, insurance underwriting, and tenant screening.\n",
      "------\n",
      "Query: Explain the dispute process under the FCRA. Which module did you used to answer this question?\n",
      "Answer:  The dispute process under the FCRA involves notifying the credit reporting agency of any errors on your credit report, providing evidence to support your dispute, and allowing the agency to investigate and correct the error if necessary. I used the FCRA module to answer this question.\n",
      "------\n",
      "Query: What obligations do credit reporting agencies have according to the FCRA? Which module did you used to answer this question?\n",
      "Answer: \n",
      "\n",
      "According to the FCRA (Fair Credit Reporting Act), credit reporting agencies have the obligation to ensure the accuracy and privacy of the information in credit reports, to investigate disputes and correct errors, and to provide consumers with access to their credit reports and scores. This information was found in Module 4: Credit Reporting and Scores.\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"What are the permissible purposes for obtaining a consumer report?\",\n",
    "    \"Explain the dispute process under the FCRA. Which module did you used to answer this question?\",\n",
    "    \"What obligations do credit reporting agencies have according to the FCRA? Which module did you used to answer this question?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    answer = qa_chain.run(query)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(\"------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
